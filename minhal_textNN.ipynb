{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering User Profiles Using Word Embeddings\n",
    "\n",
    "## Introduction\n",
    "Our objective is to find for every user a pen pal that might be the most suitable, this is a challenging problem: First we can not train a neural network \n",
    "from scratch as we do not have any labeled data, in other words, we do not have ground truth to tell us how well two users like each other. Secondly, all\n",
    "the users will have is unstructured text which is a difficult input process (any word can have a number of forms and synonyms, making word count based methods \n",
    "hard and meanigless). Luckily [unsupervised learning](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294) can solve both issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 pretrained word embeddings\n",
    "\n",
    "We want to somehow map unstructured texts into a vectors in space and then find the most similar vector \n",
    "(nearest neighbor) which will (hopefully) belong to the most similar profile.\n",
    "Luckily there are many ready made word embeddings: https://www.youtube.com/watch?v=6xPnEh_tJEc (watch until 14:50 at least, but it is all useful to know)\n",
    "\n",
    "Therefore, our first step is to load an existing word embedding. We will begin with a \n",
    "[pretrained GLOVE embedding](https://nlp.stanford.edu/projects/glove/). Specifically the glove.6B packadge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './glove.6B/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that glove.6B has embeddings into multiple different sizes. We will first begin with embedding of size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=glove_path+'6B.50.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=glove_path+'6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(glove_path+'6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(glove_path+'6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(glove_path+'/6B.50.dat')[:]\n",
    "words = pickle.load(open(glove_path+'/6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(glove_path+'/6B.50_idx.pkl', 'rb'))\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first test case will be to try to use word embeddings to classify\n",
    "movie reviews into positive and negative. This is from [Dr Lilian Lee's imdb database](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf).\n",
    "\n",
    "We will be using the ready made [K-Nearest Neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (make sure you understand this one) from teh sklearn packadge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "import nltk\n",
    "import os\n",
    "import collections\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "POS_PATH = '../../TA/bayesian/posTrain/'\n",
    "NEG_PATH = '../../TA/bayesian/negTrain/'\n",
    "POS_TEST_PATH = '../../TA/bayesian/posTest/'\n",
    "NEG_TEST_PATH = '../../TA/bayesian/negTest/'\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+[\\']*[a-zA-Z]+')\n",
    "# These are common words that do not make our vectors more sensative to the semantics of the text\n",
    "# so we remove them\n",
    "common_words = ['the','an','in','of', 'to', 'and', 'in', 'a', 'for', \\\n",
    "                'that', 'on', 'is', 'he', 'as', 'it', 'by', 'at', 'my', 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in the vocabulary and the length of each vector (will be 50 in the training example)\n",
    "NUM_WORDS = len(word2idx)\n",
    "VEC_DIM = len(glove['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-0a4583657db3>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-0a4583657db3>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    n += #......\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Given a text we want to get a vector. This functons returns a dictionary\n",
    "# of file_name and it's vector.\n",
    "def get_text_vectors(dir_path,vocab,tokenizer,VEC_DIM,common_words=[]):\n",
    "    file_list = [dir_path+f for f in os.listdir(dir_path)]\n",
    "    vec_dict = dict()\n",
    "    for file_name in file_list:\n",
    "        file_vec = np.zeros(VEC_DIM)\n",
    "        n = 0\n",
    "        with  open(file_name,'r',encoding='ISO-8859-1', errors='ignore') as f:\n",
    "            rawText = f.read()\n",
    "            counter = collections.Counter(tokenizer.tokenize(rawText))\n",
    "            for word in counter:\n",
    "                if word in common_words:\n",
    "                    continue\n",
    "                if word in vocab:\n",
    "                    # If we just add all vectors we will have unbalanced vector values\n",
    "                    # becuase some profiles are simply longer, so we normalize by the number of words.\n",
    "                    n += #......\n",
    "                    file_vec += vocab[word]*#......\n",
    "        vec_dict[file_name] = file_vec / #......\n",
    "    return vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_vec = get_file_vectors(POS_PATH,glove,tokenizer,VEC_DIM,common_words)\n",
    "neg_train_vec = get_file_vectors(NEG_PATH,glove,tokenizer,VEC_DIM,common_words)\n",
    "pos_test_vec = get_file_vectors(POS_TEST_PATH,glove,tokenizer,VEC_DIM,common_words)\n",
    "neg_test_vec = get_file_vectors(NEG_TEST_PATH,glove,tokenizer,VEC_DIM,common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(pos_train_vec.values()) + list(neg_train_vec.values())\n",
    "y_train = list(np.ones(len(pos_train_vec)))+list(np.zeros(len(neg_train_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = list(pos_test_vec.values()) + list(neg_test_vec.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(np.ones(len(pos_test_vec)))+list(np.zeros(len(neg_test_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58 32]\n",
      " [37 52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.64      0.63        90\n",
      "         1.0       0.62      0.58      0.60        89\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       179\n",
      "   macro avg       0.61      0.61      0.61       179\n",
      "weighted avg       0.61      0.61      0.61       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
